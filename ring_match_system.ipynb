{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ring Matcher Notebook\n",
    "This notebook sets up the Ring Matcher pipeline:\n",
    "- Installs dependencies\n",
    "- Loads models (SAM, CLIP, Real-ESRGAN, YOLO)\n",
    "- Uploads and indexes catalog images\n",
    "- Processes a query ring image\n",
    "- Performs rotation-invariant similarity matching"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===============================\n",
    "# 1️⃣ Setup: Install dependencies (Run once)\n",
    "# ===============================\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def install_packages():\n",
    "    subprocess.run([\"pip\", \"install\", \"torch\", \"torchvision\", \"--index-url\", \"https://download.pytorch.org/whl/cpu\"])\n",
    "    subprocess.run([\"pip\", \"install\", \"git+https://github.com/facebookresearch/segment-anything.git\"])\n",
    "    subprocess.run([\"pip\", \"install\", \"basicsr\", \"--no-deps\"])\n",
    "    subprocess.run([\"pip\", \"install\", \"gfpgan\", \"facexlib\"])\n",
    "    subprocess.run([\"pip\", \"install\", \"git+https://github.com/xinntao/Real-ESRGAN.git@v0.3.0\"])\n",
    "    subprocess.run([\"pip\", \"install\", \"git+https://github.com/openai/CLIP.git\"])\n",
    "    subprocess.run([\"pip\", \"install\", \"ultralytics\"])\n",
    "\n",
    "# install_packages()  # Uncomment to install packages"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===============================\n",
    "# 2️⃣ Imports\n",
    "# ===============================\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from realesrgan import RealESRGANer\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from ultralytics import YOLO\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Fix for torchvision.functional_tensor error\n",
    "if not hasattr(torchvision.transforms, 'functional_tensor'):\n",
    "    import torchvision.transforms.functional as F\n",
    "    torchvision.transforms.functional_tensor = F\n",
    "    print(\"Torchvision compatibility patch applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===============================\n",
    "# 3️⃣ Setup Models\n",
    "# ===============================\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load SAM\n",
    "sam_checkpoint_path = 'sam_vit_b.pth'  # Download manually or script\n",
    "sam = sam_model_registry['vit_b'](checkpoint=sam_checkpoint_path).to(device)\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "# Load CLIP\n",
    "import clip\n",
    "clip_model, clip_preprocess = clip.load('ViT-B/32', device=device)\n",
    "\n",
    "# Load Real-ESRGAN\n",
    "model_arch = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n",
    "realesrgan_model = RealESRGANer(\n",
    "    scale=4,\n",
    "    model_path='https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth',\n",
    "    model=model_arch,\n",
    "    tile=0,\n",
    "    tile_pad=10,\n",
    "    pre_pad=0,\n",
    "    half=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# Load YOLO\n",
    "yolo_model = YOLO('yolo11m.pt')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===============================\n",
    "# 4️⃣ Helper Functions (Masking, Embeddings, Extraction)\n",
    "# ===============================\n",
    "def get_sam_mask(image_bgr):\n",
    "    h, w = image_bgr.shape[:2]\n",
    "    predictor.set_image(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB))\n",
    "    masks, _, _ = predictor.predict(\n",
    "        point_coords=np.array([[w//2, h//2]]),\n",
    "        point_labels=np.array([1]),\n",
    "        multimask_output=False\n",
    "    )\n",
    "    return masks[0]\n",
    "\n",
    "def process_for_clip(image_bgr, mask=None):\n",
    "    if mask is not None:\n",
    "        image_bgr[mask == 0] = [255, 255, 255]\n",
    "    img_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(img_rgb).resize((224,224), Image.LANCZOS)\n",
    "\n",
    "def get_embedding(pil_img):\n",
    "    img_input = clip_preprocess(pil_img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = clip_model.encode_image(img_input)\n",
    "    emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "    return emb.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===============================\n",
    "# 5️⃣ Rotation-Invariant Matching\n",
    "# ===============================\n",
    "def rotation_invariant_matching(query_embs, db_embs, top_k=5, threshold=0.65):\n",
    "    sims = cosine_similarity(query_embs, db_embs)\n",
    "    best_sim = sims.max(axis=0)\n",
    "    idx = np.where(best_sim >= threshold)[0]\n",
    "    idx = idx[np.argsort(best_sim[idx])[::-1]]\n",
    "    return idx[:top_k], best_sim[idx[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===============================\n",
    "# 6️⃣ Main Pipeline (Provide Paths)\n",
    "# ===============================\n",
    "upload_folder = './uploads'\n",
    "db_folder = './catalog'\n",
    "os.makedirs(upload_folder, exist_ok=True)\n",
    "os.makedirs(db_folder, exist_ok=True)\n",
    "\n",
    "# Users must place catalog images in './catalog' and query image in './uploads'\n",
    "query_path = './uploads/query_ring.jpg'\n",
    "\n",
    "# Load query image\n",
    "original_img = Image.open(query_path).convert('RGB')\n",
    "\n",
    "# Process query (clean or extract)\n",
    "def is_clean_catalog_image(pil_img):\n",
    "    img = np.array(pil_img.convert('RGB'))\n",
    "    h, w, _ = img.shape\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    white_ratio = np.mean(gray > 240)\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    skin_mask = cv2.inRange(hsv, (0,20,70),(20,255,255))\n",
    "    skin_ratio = np.mean(skin_mask>0)\n",
    "    _, thresh = cv2.threshold(gray, 245, 255, cv2.THRESH_BINARY_INV)\n",
    "    object_ratio = np.mean(thresh>0)\n",
    "    return white_ratio>0.55 and skin_ratio<0.005 and object_ratio<0.6\n",
    "\n",
    "if is_clean_catalog_image(original_img):\n",
    "    extracted_pil = original_img\n",
    "else:\n",
    "    # Use your extracted_actual_ring function\n",
    "    extracted_pil = extracted_actual_ring(query_path, predictor, realesrgan_model, yolo_model)\n",
    "    if extracted_pil is None:\n",
    "        print('Fallback to center crop')\n",
    "        # fallback_center_crop(query_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
