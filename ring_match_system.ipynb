{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcjBSapQA2oTc3gfdWR+zj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hafsaimranattaria71125/Ring_matcher/blob/main/ring_match_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKFYCK_Z4gP3"
      },
      "outputs": [],
      "source": [
        "# 1. Clean folders\n",
        "!rm -rf /content/ring_matcher\n",
        "!mkdir -p /content/ring_matcher/data/ab_jewellers/images\n",
        "\n",
        "# 2. CPU-Specific installs (Fast & Light)\n",
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "\n",
        "# 3. Real-ESRGAN without the 'Basicsr' headache\n",
        "!pip install basicsr --no-deps\n",
        "!pip install gfpgan facexlib\n",
        "!pip install git+https://github.com/xinntao/Real-ESRGAN.git@v0.3.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the ViT-B checkpoint from Hugging Face\n",
        "!wget https://huggingface.co/ybelkada/segment-anything/resolve/main/checkpoints/sam_vit_b_01ec64.pth\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "ZIniBiny5i8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics"
      ],
      "metadata": {
        "id": "Lliqz0gWSj87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix for the torchvision.transforms.functional_tensor error\n",
        "import torchvision\n",
        "import torch\n",
        "\n",
        "if not hasattr(torchvision.transforms, 'functional_tensor'):\n",
        "    import torchvision.transforms.functional as F\n",
        "    torchvision.transforms.functional_tensor = F\n",
        "    print(\"Torchvision compatibility patch applied.\")"
      ],
      "metadata": {
        "id": "EQgNbXCC9Wbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# COMPLETE SETUP FOR RING MATCHER\n",
        "# ===============================\n",
        "\n",
        "print(\"ðŸ”§ SETTING UP RING MATCHER...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Download SAM weights\n",
        "print(\"\\nðŸ“¥ Downloading SAM weights...\")\n",
        "import os\n",
        "if os.path.exists(\"sam_vit_b.pth\"):\n",
        "    os.remove(\"sam_vit_b.pth\")\n",
        "\n",
        "!curl -L -o sam_vit_b.pth \"https://huggingface.co/ybelkada/segment-anything/resolve/main/checkpoints/sam_vit_b_01ec64.pth\"\n",
        "\n",
        "if os.path.exists(\"sam_vit_b.pth\"):\n",
        "    file_size = os.path.getsize(\"sam_vit_b.pth\") / (1024 * 1024)\n",
        "    print(f\"âœ… SAM weights: {file_size:.2f} MB\")\n",
        "else:\n",
        "    print(\"âŒ SAM download failed!\")\n",
        "\n",
        "# 2. Clear folders\n",
        "print(\"\\nðŸ—‘ï¸  Clearing previous data...\")\n",
        "import shutil\n",
        "folders = [\"/content/ring_matcher/data/uploads\", \"/content/ring_matcher/data/ab_jewellers/images\"]\n",
        "for folder in folders:\n",
        "    if os.path.exists(folder):\n",
        "        shutil.rmtree(folder)\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "print(\"âœ… Setup complete!\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "2Fm8Wh9Xk5vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "# This command finds the broken file and replaces the old import with the new one\n",
        "!sed -i 's/from torchvision.transforms.functional_tensor import rgb_to_grayscale/from torchvision.transforms.functional import rgb_to_grayscale/' /usr/local/lib/python3.12/dist-packages/basicsr/data/degradations.py\n",
        "\n",
        "print(\"âœ… Patch applied! 'functional_tensor' error should be gone.\")"
      ],
      "metadata": {
        "id": "VIiv5_KkFBei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2, torch, clip, numpy as np\n",
        "from PIL import Image\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "from realesrgan import RealESRGANer\n",
        "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
        "\n",
        "# ===============================\n",
        "# 1ï¸âƒ£ SETUP MODELS\n",
        "# ===============================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load SAM\n",
        "sam = sam_model_registry[\"vit_b\"](checkpoint=\"sam_vit_b.pth\").to(device)\n",
        "predictor = SamPredictor(sam)\n",
        "\n",
        "# Load CLIP\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# --- NEW: Setup Real-ESRGAN ---\n",
        "# This defines the architecture for the \"x4plus\" model\n",
        "model_arch = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n",
        "\n",
        "# This initializes the actual upscaler object\n",
        "realesrgan_model = RealESRGANer(\n",
        "    scale=4,\n",
        "    model_path='https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth',\n",
        "    model=model_arch,\n",
        "    tile=0,\n",
        "    tile_pad=10,\n",
        "    pre_pad=0,\n",
        "    half=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(f\"âœ… All models (SAM, CLIP, Real-ESRGAN) loaded on {device}\")"
      ],
      "metadata": {
        "id": "69Pj9NkbEN6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "# --- INITIALIZE MODELS ONCE ---\n",
        "def load_models(sam_checkpoint_path):\n",
        "    # 1. YOLO for Jewelry Detection\n",
        "    # 'yolo11m.pt' or 'yolo26m.pt' for high-precision ring detection\n",
        "    yolo_model = YOLO(\"yolo11m.pt\")\n",
        "\n",
        "    # 2. SAM for Surgical Extraction\n",
        "    sam = sam_model_registry[\"vit_h\"](checkpoint=sam_checkpoint_path)\n",
        "    predictor = SamPredictor(sam)\n",
        "\n",
        "    # 3. Real-ESRGAN (Optional)\n",
        "    # Ensure you have the realesrgan library installed and model loaded if needed\n",
        "\n",
        "    return yolo_model, predictor\n",
        "\n",
        "# Example usage:\n",
        "# yolo_model, predictor = load_models(\"/content/sam_vit_h_4b8939.pth\")"
      ],
      "metadata": {
        "id": "wIdsbQy4SLb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2, torch, clip, numpy as np\n",
        "from PIL import Image\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "from realesrgan import RealESRGANer\n",
        "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
        "from google.colab import files # Added this line\n",
        "\n",
        "# ===============================\n",
        "# 2ï¸âƒ£ PATH SETUP\n",
        "# ===============================\n",
        "db_folder = \"/content/ring_matcher/data/ab_jewellers/images\"\n",
        "os.makedirs(db_folder, exist_ok=True)\n",
        "\n",
        "# ===============================\n",
        "# 3ï¸âƒ£ UPLOAD CATALOG IMAGES\n",
        "# ===============================\n",
        "print(\"ðŸ“¤ Upload catalog ring images (multiple allowed)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if len(uploaded) == 0:\n",
        "    raise RuntimeError(\"âŒ No images uploaded.\")\n",
        "\n",
        "for fn in uploaded:\n",
        "    shutil.move(fn, os.path.join(db_folder, fn))\n",
        "\n",
        "print(f\"âœ… Uploaded {len(uploaded)} images to catalog\")\n",
        "\n",
        "# ===============================\n",
        "# 4ï¸âƒ£ HELPER FUNCTIONS\n",
        "# ===============================\n",
        "def get_sam_mask(image_bgr):\n",
        "    h, w = image_bgr.shape[:2]\n",
        "    predictor.set_image(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB))\n",
        "    masks, _, _ = predictor.predict(\n",
        "        point_coords=np.array([[w // 2, h // 2]]),\n",
        "        point_labels=np.array([1]),\n",
        "        multimask_output=False\n",
        "    )\n",
        "    return masks[0]\n",
        "\n",
        "def process_for_clip(image_bgr, mask=None):\n",
        "    if mask is not None:\n",
        "        image_bgr[mask == 0] = [255, 255, 255]\n",
        "    img_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb).resize((224, 224), Image.LANCZOS)\n",
        "\n",
        "def get_embedding(pil_img):\n",
        "    img_input = clip_preprocess(pil_img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        emb = clip_model.encode_image(img_input)\n",
        "    emb = emb / emb.norm(dim=-1, keepdim=True)\n",
        "    return emb.cpu().numpy()\n",
        "\n",
        "# ===============================\n",
        "# 5ï¸âƒ£ INDEX CATALOG\n",
        "# ===============================\n",
        "db_images = sorted([\n",
        "    f for f in os.listdir(db_folder)\n",
        "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
        "])\n",
        "\n",
        "print(f\"ðŸ” Found {len(db_images)} catalog images\")\n",
        "\n",
        "catalog_embs = []\n",
        "valid_names = []\n",
        "\n",
        "for name in db_images:\n",
        "    print(f\"   Indexing {name}...\")\n",
        "    img_path = os.path.join(db_folder, name)\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    if img is None:\n",
        "        print(f\"âš ï¸ Skipping unreadable image: {name}\")\n",
        "        continue\n",
        "\n",
        "    # ðŸ”¥ APPLY SAME SEGMENTATION AS QUERY\n",
        "    pil_ring = process_for_clip(img)  # no mask\n",
        "\n",
        "\n",
        "    emb = get_embedding(pil_ring)\n",
        "    catalog_embs.append(emb)\n",
        "    valid_names.append(name)\n",
        "\n",
        "if len(catalog_embs) == 0:\n",
        "    raise RuntimeError(\"âŒ No valid images indexed.\")\n",
        "\n",
        "db_embs = np.vstack(catalog_embs)\n",
        "\n",
        "print(f\"âœ… Indexed {db_embs.shape[0]} catalog images\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pa8NVeMDqsfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extracted_actual_ring(image_path, predictor, realesrgan_model=None, yolo_model=None):\n",
        "    \"\"\"\n",
        "    Robust ring extraction for similarity matching.\n",
        "    - NO rotation\n",
        "    - NO ellipse fitting\n",
        "    - Preserves geometry & thickness\n",
        "    \"\"\"\n",
        "\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    from PIL import Image\n",
        "\n",
        "    # =======================\n",
        "    # LOAD IMAGE\n",
        "    # =======================\n",
        "    bgr = cv2.imread(image_path)\n",
        "    if bgr is None:\n",
        "        return None\n",
        "\n",
        "    h, w = bgr.shape[:2]\n",
        "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # =======================\n",
        "    # YOLO ROI (optional)\n",
        "    # =======================\n",
        "    ring_roi = np.array([w*0.25, h*0.25, w*0.75, h*0.75], dtype=np.float32)\n",
        "\n",
        "    if yolo_model:\n",
        "        results = yolo_model.predict(bgr, conf=0.25, verbose=False)\n",
        "        if results and len(results[0].boxes) > 0:\n",
        "            boxes = results[0].boxes.xyxy.cpu().numpy()\n",
        "            areas = (boxes[:,2]-boxes[:,0])*(boxes[:,3]-boxes[:,1])\n",
        "            ring_roi = boxes[np.argmin(areas)]\n",
        "\n",
        "    ring_roi = ring_roi.astype(int)\n",
        "    x1, y1, x2, y2 = ring_roi\n",
        "\n",
        "    predictor.set_image(rgb)\n",
        "\n",
        "    # =======================\n",
        "    # SAM PROMPTS\n",
        "    # =======================\n",
        "    cx, cy = (x1+x2)//2, (y1+y2)//2\n",
        "    rw, rh = x2-x1, y2-y1\n",
        "\n",
        "    pts = np.array([\n",
        "        [cx, cy],\n",
        "        [cx+rw*0.15, cy],\n",
        "        [cx-rw*0.15, cy],\n",
        "        [cx, cy+rh*0.15],\n",
        "        [cx, cy-rh*0.15],\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    labels = np.ones(len(pts), dtype=np.int32)\n",
        "\n",
        "    masks, scores, _ = predictor.predict(\n",
        "        point_coords=pts,\n",
        "        point_labels=labels,\n",
        "        box=ring_roi[None, :],\n",
        "        multimask_output=True\n",
        "    )\n",
        "\n",
        "    # =======================\n",
        "    # PICK BEST MASK\n",
        "    # =======================\n",
        "    best_mask = None\n",
        "    roi_area = rw * rh\n",
        "\n",
        "    for m in masks:\n",
        "        area = np.sum(m)\n",
        "        ratio = area / roi_area\n",
        "        if 0.001 < ratio < 0.30:\n",
        "            best_mask = (m * 255).astype(np.uint8)\n",
        "            break\n",
        "\n",
        "    if best_mask is None:\n",
        "        return None\n",
        "\n",
        "    # =======================\n",
        "    # REMOVE FINGER (DISTANCE-BASED)\n",
        "    # =======================\n",
        "    dist = cv2.distanceTransform(best_mask, cv2.DIST_L2, 5)\n",
        "    thickness = np.percentile(dist[best_mask > 0], 70)\n",
        "\n",
        "    # Keep only regions with ring-like thickness\n",
        "    ring_only = np.zeros_like(best_mask)\n",
        "    ring_only[dist >= thickness * 0.55] = 255\n",
        "\n",
        "    # Cleanup\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\n",
        "    ring_only = cv2.morphologyEx(ring_only, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
        "\n",
        "    # Largest component only\n",
        "    num, labels, stats, _ = cv2.connectedComponentsWithStats(ring_only)\n",
        "    if num <= 1:\n",
        "        return None\n",
        "\n",
        "    largest = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])\n",
        "    ring_only = (labels == largest).astype(np.uint8) * 255\n",
        "\n",
        "    # =======================\n",
        "    # EXTRACT RGBA (NO ROTATION)\n",
        "    # =======================\n",
        "    bgra = cv2.cvtColor(bgr, cv2.COLOR_BGR2BGRA)\n",
        "    bgra[:,:,3] = ring_only\n",
        "\n",
        "    nz = cv2.findNonZero(ring_only)\n",
        "    if nz is None:\n",
        "        return None\n",
        "\n",
        "    x, y, bw, bh = cv2.boundingRect(nz)\n",
        "    pad = 12\n",
        "    crop = bgra[\n",
        "        max(0,y-pad):y+bh+pad,\n",
        "        max(0,x-pad):x+bw+pad\n",
        "    ]\n",
        "\n",
        "    # =======================\n",
        "    # FINAL CANVAS (512x512)\n",
        "    # =======================\n",
        "    canvas = np.ones((512,512,3), dtype=np.uint8) * 255\n",
        "\n",
        "    ch, cw = crop.shape[:2]\n",
        "    scale = min(420/max(ch,cw), 1.0)\n",
        "    crop = cv2.resize(crop, None, fx=scale, fy=scale, interpolation=cv2.INTER_LANCZOS4)\n",
        "\n",
        "    ch, cw = crop.shape[:2]\n",
        "    y0, x0 = (512-ch)//2, (512-cw)//2\n",
        "\n",
        "    alpha = crop[:,:,3]/255.0\n",
        "    canvas[y0:y0+ch, x0:x0+cw] = (\n",
        "        alpha[...,None]*crop[:,:,:3] +\n",
        "        (1-alpha[...,None])*255\n",
        "    ).astype(np.uint8)\n",
        "\n",
        "    return Image.fromarray(canvas)"
      ],
      "metadata": {
        "id": "40jj3ExAXlZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "import os, shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# ===============================\n",
        "# LOAD YOLO\n",
        "# ===============================\n",
        "yolo_model = YOLO(\"yolo11m.pt\")\n",
        "print(\"âœ… YOLO model loaded.\")\n",
        "\n",
        "# ===============================\n",
        "# ROTATION UTILITY\n",
        "# ===============================\n",
        "def rotate_image_pil(img, angle):\n",
        "    return img.rotate(angle, expand=True, fillcolor=(255,255,255))\n",
        "def fallback_center_crop(image_path):\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    w, h = img.size\n",
        "    s = int(min(w, h) * 0.6)\n",
        "\n",
        "    cx, cy = w // 2, h // 2\n",
        "    crop = img.crop((cx - s//2, cy - s//2, cx + s//2, cy + s//2))\n",
        "\n",
        "    canvas = Image.new(\"RGB\", (512, 512), (255,255,255))\n",
        "    crop = crop.resize((400,400), Image.LANCZOS)\n",
        "    canvas.paste(crop, ((512-400)//2, (512-400)//2))\n",
        "    return canvas\n",
        "#Function for clean images\n",
        "def is_clean_catalog_image(pil_img):\n",
        "\n",
        "    img = np.array(pil_img.convert(\"RGB\"))\n",
        "    h, w, _ = img.shape\n",
        "\n",
        "    # --- 1ï¸âƒ£ Background whiteness ---\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    white_ratio = np.mean(gray > 240)  # near-white pixels\n",
        "\n",
        "    # --- 2ï¸âƒ£ Skin color detection (HSV) ---\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
        "    skin_mask = cv2.inRange(\n",
        "        hsv,\n",
        "        (0, 20, 70),    # lower skin HSV\n",
        "        (20, 255, 255)  # upper skin HSV\n",
        "    )\n",
        "    skin_ratio = np.mean(skin_mask > 0)\n",
        "\n",
        "    # --- 3ï¸âƒ£ Object coverage (ring size) ---\n",
        "    _, thresh = cv2.threshold(gray, 245, 255, cv2.THRESH_BINARY_INV)\n",
        "    object_ratio = np.mean(thresh > 0)\n",
        "\n",
        "    print(\n",
        "        f\"ðŸ§ª white={white_ratio:.2f}, \"\n",
        "        f\"skin={skin_ratio:.4f}, \"\n",
        "        f\"object={object_ratio:.2f}\"\n",
        "    )\n",
        "\n",
        "    # --- Decision ---\n",
        "    if white_ratio > 0.55 and skin_ratio < 0.005 and object_ratio < 0.6:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "# ===============================\n",
        "# ROTATION-INVARIANT MATCHING (FIXED)\n",
        "# ===============================\n",
        "def rotation_invariant_matching(query_embs, db_embs, top_k=5, threshold=0.65):\n",
        "    \"\"\"\n",
        "    True rotation-invariant cosine matching.\n",
        "    Assumes ALL embeddings are L2-normalized.\n",
        "    \"\"\"\n",
        "    sims = cosine_similarity(query_embs, db_embs)   # (R, N)\n",
        "    best_sim = sims.max(axis=0)                      # best rotation per DB\n",
        "\n",
        "    idx = np.where(best_sim >= threshold)[0]\n",
        "    idx = idx[np.argsort(best_sim[idx])[::-1]]\n",
        "\n",
        "    return idx[:top_k], best_sim[idx[:top_k]]\n",
        "\n",
        "# ===============================\n",
        "# MAIN PIPELINE\n",
        "# ===============================\n",
        "print(\"\\nðŸ—‘ï¸ Clearing previous data...\")\n",
        "upload_folder = \"/content/ring_matcher/data/uploads\"\n",
        "if os.path.exists(upload_folder):\n",
        "    shutil.rmtree(upload_folder)\n",
        "os.makedirs(upload_folder, exist_ok=True)\n",
        "\n",
        "print(\"--- UPLOAD YOUR RING IMAGE ---\")\n",
        "uploaded = files.upload()\n",
        "if len(uploaded) != 1:\n",
        "    raise ValueError(\"Upload exactly ONE image\")\n",
        "\n",
        "file_name = list(uploaded.keys())[0]\n",
        "query_path = os.path.join(upload_folder, file_name)\n",
        "shutil.move(file_name, query_path)\n",
        "\n",
        "print(f\"âœ… Query image: {file_name}\")\n",
        "\n",
        "# ===============================\n",
        "# PROCESS QUERY\n",
        "# ===============================\n",
        "\n",
        "original_img = Image.open(query_path).convert(\"RGB\")\n",
        "\n",
        "if is_clean_catalog_image(original_img):\n",
        "    print(\"ðŸŸ¢ Clean ring detected â€” skipping extraction\")\n",
        "    extracted_pil = original_img\n",
        "\n",
        "else:\n",
        "    print(\"ðŸŸ  Hand image detected â€” extracting ring\")\n",
        "    extracted_pil = extract_actual_ring(\n",
        "        query_path, predictor, realesrgan_model, yolo_model\n",
        "    )\n",
        "\n",
        "    if extracted_pil is None:\n",
        "        print(\"âš ï¸ Ring extraction failed â€” using fallback crop\")\n",
        "        extracted_pil = fallback_center_crop(query_path)\n",
        "\n",
        "# ===============================\n",
        "# EMBEDDINGS (IMPORTANT FIX)\n",
        "# ===============================\n",
        "print(\"ðŸ§  Computing rotation-invariant embeddings...\")\n",
        "\n",
        "# ðŸ”‘ THESE ANGLES ARE CORRECT (8 was harmful)\n",
        "angles = [0, 60, 120, 180]\n",
        "\n",
        "query_embs = []\n",
        "for a in angles:\n",
        "    rot = rotate_image_pil(extracted_pil, a)\n",
        "    emb = get_embedding(rot).astype(np.float32)\n",
        "    emb /= np.linalg.norm(emb) + 1e-8   # âœ… CRITICAL\n",
        "    query_embs.append(emb)\n",
        "\n",
        "query_embs = np.vstack(query_embs)\n",
        "\n",
        "# normalize DB embeddings ONCE\n",
        "db_embs = db_embs.astype(np.float32)\n",
        "db_embs /= np.linalg.norm(db_embs, axis=1, keepdims=True) + 1e-8\n",
        "\n",
        "# ===============================\n",
        "# MATCHING\n",
        "# ===============================\n",
        "print(\"ðŸ”— Matching against database...\")\n",
        "top_indices, scores = rotation_invariant_matching(\n",
        "    query_embs, db_embs, top_k=4, threshold=0.65\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# VISUALIZATION\n",
        "# ===============================\n",
        "plt.figure(figsize=(24,4))\n",
        "\n",
        "plt.subplot(1,6,1)\n",
        "plt.imshow(original_img)\n",
        "plt.title(\"ORIGINAL\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1,6,2)\n",
        "plt.imshow(extracted_pil)\n",
        "plt.title(\"EXTRACTED RING\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "for i, idx in enumerate(top_indices):\n",
        "    match_img = Image.open(os.path.join(db_folder, db_images[idx])).convert(\"RGB\")\n",
        "    score = scores[i]\n",
        "\n",
        "    plt.subplot(1,6,i+3)\n",
        "    plt.imshow(match_img)\n",
        "\n",
        "    if score >= 0.85:\n",
        "        label = \"âœ… EXACT\"\n",
        "        color = \"limegreen\"\n",
        "    elif score >= 0.75:\n",
        "        label = \"ðŸ”¥ VERY SIMILAR\"\n",
        "        color = \"gold\"\n",
        "    else:\n",
        "        label = \"ðŸ‘¥ SIMILAR\"\n",
        "        color = \"orange\"\n",
        "\n",
        "    plt.title(f\"{label}\\n{db_images[idx][:18]}...\\n{score:.3f}\",\n",
        "              fontsize=9, color=color, fontweight=\"bold\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Matching complete.\")"
      ],
      "metadata": {
        "id": "N5I56PQmY6A5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}